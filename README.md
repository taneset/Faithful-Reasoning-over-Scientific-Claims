

## Faithful-Reasoning-over-Scientific-Claims

### What is Faithful-Reasoning-over-Scientific-Claims?
Ensuring accurate and reliable verification of scientific statements through faithful reasoning over claims involves incorporating relevant knowledge from existing literature. This approach aims to address challenges, such as misinformation, by considering multiple perspectives and implicit assumptions within scientific claims.

By distilling information from diverse scientific abstracts and weighing them based on factors like reputation and citation count, this method enhances the ability to trace evidence back to its original sources. It provides a more comprehensive understanding of complex topics, thus enabling more informed decision-making in scientific domains. This approach also helps to avoid the hallucinations that language models, such as the following example generated by the May 2023 version of GPT 3.5, none of the given references exist:

<p align="center">
         <img width="800" src="reason/PNG/GPThalex.png" alt="drawing">
</p>
### How does it work?
Through multiple stages, the framework enables faithful reasoning over scientific claims by providing a traceable provenance from global explanations to evidence abstracts. It makes inferences in multiple steps to ensure the article's relevance to the given claim. This multistep process provides an explanation and a verdict outcome, which can be traced back to the source document for each sentence included in the explanation.
<p align="center">
         <img width="500" src="reason/PNG/high.png" alt="drawing">
</p>


In more detail, the framework has seven steps. In step 1 and 2, we retrieve relevant scientific abstracts for the claim and the anti-claim. In Step 3, we generate claim-specific takeaways from these retrieved abstracts, and prune abstracts with weak takeaways. Step 4 generates a local label and its explanation for each takeaway. In Step 5, local labels are aggregated and weighted based on the label score and article reputation. In Step 6, a detailed summary of the local explanations with respect to the given claim is generated, which we call the global explanation. Finally, in Step 7, we trace back each sentence in the global explanation to sentences in the abstracts to show its attribution.


<p align="center">
         <img width="1000" src="reason/PNG/detailwide.png" alt="drawing">
</p>
### Using EMMA (Demo video)

Watch a 2 minutes short video for a demonstration EMMA.

<iframe width="700px" height="500px" src="https://user-images.githubusercontent.com/22459345/183147600-540e0552-8d8a-482d-acd3-b0230fbe4db2.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>



### Try out EMMA!
Click [`here`](https://allenai-defeasible-explanations-srcvdemo-interactive-jpe7t4.streamlitapp.com/?on_demand=false) to try out EMMA!
